{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Introduction to Human Activity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will explore some common ways of preprocessing human activity recognition data.\n",
    "\n",
    "Using the example data we will learn:\n",
    "* how to merge multiple files into one large DataFrame\n",
    "* how to split data into sliding windows\n",
    "* how to quickly extract features from a window\n",
    "* how to set the number of classes considered for classification\n",
    "* how to build a simple Random Forest Classifier and train it on HAR data\n",
    "* how to build a simple CNN and train it on HAR data \n",
    "\n",
    "Bear in mind that the sample data offered is not cleaned or high quality. You should not use it in your own experiments but it is useful for this tutorial.\n",
    "\n",
    "You will need the following packages: \n",
    "* tsfresh\n",
    "* scikit-learn\n",
    "* tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current Model Note for cw3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tsfresh\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# keras goodies\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Conv1D, Dropout, MaxPooling1D, BatchNormalization, LSTM\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import metrics as kmetrics\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading multiple files into one large DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage you should only be working with clean data, saved in the format required for Coursework 1. An example of such data can be found in the Data/Clean/ folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEPRECATED: New clean file was generated, which we will be used from now on\n",
    "# base_df = pd.DataFrame()\n",
    "\n",
    "# clean_data_folder = \"../pdiot-data/2021\"\n",
    "\n",
    "# for sUNN_folder in os.listdir(clean_data_folder):\n",
    "#     print(sUNN_folder)\n",
    "#     try:\n",
    "#         for filename in os.listdir(clean_data_folder+\"/\"+sUNN_folder):\n",
    "            \n",
    "#             full_path = f\"{clean_data_folder}/{sUNN_folder}/{filename}\"\n",
    "# #             print(full_path)\n",
    "\n",
    "#             # load data into a DataFrame\n",
    "#             new_df = pd.read_csv(full_path)\n",
    "\n",
    "#             # merge into the base DataFrame\n",
    "#             base_df = pd.concat([base_df, new_df])\n",
    "#     except:\n",
    "#         print(sUNN_folder, \" is not a folder\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Wassim/miniconda3/envs/pdiot/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3441: DtypeWarning: Columns (11) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "#Load Clean Respeck Recording to base_df\n",
    "clean_data_folder = \"../pdiot-data/2021/Respeck_recordings_clean.csv\"\n",
    "base_df = pd.read_csv(clean_data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>accel_x</th>\n",
       "      <th>accel_y</th>\n",
       "      <th>accel_z</th>\n",
       "      <th>gyro_x</th>\n",
       "      <th>gyro_y</th>\n",
       "      <th>gyro_z</th>\n",
       "      <th>sensor_type</th>\n",
       "      <th>activity_type</th>\n",
       "      <th>activity_code</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>notes</th>\n",
       "      <th>recording_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.633516e+12</td>\n",
       "      <td>0.261475</td>\n",
       "      <td>-1.116516</td>\n",
       "      <td>-0.502991</td>\n",
       "      <td>-0.812500</td>\n",
       "      <td>12.312500</td>\n",
       "      <td>19.500000</td>\n",
       "      <td>Respeck</td>\n",
       "      <td>Climbing stairs</td>\n",
       "      <td>12.0</td>\n",
       "      <td>s1870467</td>\n",
       "      <td>rob</td>\n",
       "      <td>Respeck_s1870467_Climbing stairs_06-10-2021_11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.633516e+12</td>\n",
       "      <td>-0.177734</td>\n",
       "      <td>-0.636292</td>\n",
       "      <td>-0.477600</td>\n",
       "      <td>-5.265625</td>\n",
       "      <td>2.953125</td>\n",
       "      <td>-9.281250</td>\n",
       "      <td>Respeck</td>\n",
       "      <td>Climbing stairs</td>\n",
       "      <td>12.0</td>\n",
       "      <td>s1870467</td>\n",
       "      <td>rob</td>\n",
       "      <td>Respeck_s1870467_Climbing stairs_06-10-2021_11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.633516e+12</td>\n",
       "      <td>0.351562</td>\n",
       "      <td>-0.879456</td>\n",
       "      <td>-0.335754</td>\n",
       "      <td>5.671875</td>\n",
       "      <td>24.656250</td>\n",
       "      <td>-10.562500</td>\n",
       "      <td>Respeck</td>\n",
       "      <td>Climbing stairs</td>\n",
       "      <td>12.0</td>\n",
       "      <td>s1870467</td>\n",
       "      <td>rob</td>\n",
       "      <td>Respeck_s1870467_Climbing stairs_06-10-2021_11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.633516e+12</td>\n",
       "      <td>0.298584</td>\n",
       "      <td>-1.253479</td>\n",
       "      <td>-0.299622</td>\n",
       "      <td>-3.609375</td>\n",
       "      <td>2.687500</td>\n",
       "      <td>5.890625</td>\n",
       "      <td>Respeck</td>\n",
       "      <td>Climbing stairs</td>\n",
       "      <td>12.0</td>\n",
       "      <td>s1870467</td>\n",
       "      <td>rob</td>\n",
       "      <td>Respeck_s1870467_Climbing stairs_06-10-2021_11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.633516e+12</td>\n",
       "      <td>0.056152</td>\n",
       "      <td>-0.777405</td>\n",
       "      <td>-0.210754</td>\n",
       "      <td>-14.328125</td>\n",
       "      <td>5.421875</td>\n",
       "      <td>8.140625</td>\n",
       "      <td>Respeck</td>\n",
       "      <td>Climbing stairs</td>\n",
       "      <td>12.0</td>\n",
       "      <td>s1870467</td>\n",
       "      <td>rob</td>\n",
       "      <td>Respeck_s1870467_Climbing stairs_06-10-2021_11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492671</th>\n",
       "      <td>1.632930e+12</td>\n",
       "      <td>-0.376465</td>\n",
       "      <td>-0.746399</td>\n",
       "      <td>0.693543</td>\n",
       "      <td>-0.171875</td>\n",
       "      <td>-0.953125</td>\n",
       "      <td>1.281250</td>\n",
       "      <td>Respeck</td>\n",
       "      <td>Sitting bent backward</td>\n",
       "      <td>5.0</td>\n",
       "      <td>s2211228</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Respeck_s2211228_Sitting bent backward_29-09-2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492672</th>\n",
       "      <td>1.632930e+12</td>\n",
       "      <td>-0.345703</td>\n",
       "      <td>-0.749329</td>\n",
       "      <td>0.677429</td>\n",
       "      <td>-0.671875</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>-1.015625</td>\n",
       "      <td>Respeck</td>\n",
       "      <td>Sitting bent backward</td>\n",
       "      <td>5.0</td>\n",
       "      <td>s2211228</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Respeck_s2211228_Sitting bent backward_29-09-2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492673</th>\n",
       "      <td>1.632930e+12</td>\n",
       "      <td>-0.368408</td>\n",
       "      <td>-0.745422</td>\n",
       "      <td>0.690857</td>\n",
       "      <td>-0.218750</td>\n",
       "      <td>-0.640625</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>Respeck</td>\n",
       "      <td>Sitting bent backward</td>\n",
       "      <td>5.0</td>\n",
       "      <td>s2211228</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Respeck_s2211228_Sitting bent backward_29-09-2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492674</th>\n",
       "      <td>1.632930e+12</td>\n",
       "      <td>-0.363281</td>\n",
       "      <td>-0.747375</td>\n",
       "      <td>0.689148</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.796875</td>\n",
       "      <td>-0.046875</td>\n",
       "      <td>Respeck</td>\n",
       "      <td>Sitting bent backward</td>\n",
       "      <td>5.0</td>\n",
       "      <td>s2211228</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Respeck_s2211228_Sitting bent backward_29-09-2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492675</th>\n",
       "      <td>1.632930e+12</td>\n",
       "      <td>-0.369873</td>\n",
       "      <td>-0.734436</td>\n",
       "      <td>0.684753</td>\n",
       "      <td>-0.140625</td>\n",
       "      <td>-0.531250</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>Respeck</td>\n",
       "      <td>Sitting bent backward</td>\n",
       "      <td>5.0</td>\n",
       "      <td>s2211228</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Respeck_s2211228_Sitting bent backward_29-09-2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>492676 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           timestamp   accel_x   accel_y   accel_z     gyro_x     gyro_y  \\\n",
       "0       1.633516e+12  0.261475 -1.116516 -0.502991  -0.812500  12.312500   \n",
       "1       1.633516e+12 -0.177734 -0.636292 -0.477600  -5.265625   2.953125   \n",
       "2       1.633516e+12  0.351562 -0.879456 -0.335754   5.671875  24.656250   \n",
       "3       1.633516e+12  0.298584 -1.253479 -0.299622  -3.609375   2.687500   \n",
       "4       1.633516e+12  0.056152 -0.777405 -0.210754 -14.328125   5.421875   \n",
       "...              ...       ...       ...       ...        ...        ...   \n",
       "492671  1.632930e+12 -0.376465 -0.746399  0.693543  -0.171875  -0.953125   \n",
       "492672  1.632930e+12 -0.345703 -0.749329  0.677429  -0.671875  -0.187500   \n",
       "492673  1.632930e+12 -0.368408 -0.745422  0.690857  -0.218750  -0.640625   \n",
       "492674  1.632930e+12 -0.363281 -0.747375  0.689148   0.078125   0.796875   \n",
       "492675  1.632930e+12 -0.369873 -0.734436  0.684753  -0.140625  -0.531250   \n",
       "\n",
       "           gyro_z sensor_type          activity_type  activity_code  \\\n",
       "0       19.500000     Respeck        Climbing stairs           12.0   \n",
       "1       -9.281250     Respeck        Climbing stairs           12.0   \n",
       "2      -10.562500     Respeck        Climbing stairs           12.0   \n",
       "3        5.890625     Respeck        Climbing stairs           12.0   \n",
       "4        8.140625     Respeck        Climbing stairs           12.0   \n",
       "...           ...         ...                    ...            ...   \n",
       "492671   1.281250     Respeck  Sitting bent backward            5.0   \n",
       "492672  -1.015625     Respeck  Sitting bent backward            5.0   \n",
       "492673   0.375000     Respeck  Sitting bent backward            5.0   \n",
       "492674  -0.046875     Respeck  Sitting bent backward            5.0   \n",
       "492675   0.687500     Respeck  Sitting bent backward            5.0   \n",
       "\n",
       "       subject_id notes                                       recording_id  \n",
       "0        s1870467   rob  Respeck_s1870467_Climbing stairs_06-10-2021_11...  \n",
       "1        s1870467   rob  Respeck_s1870467_Climbing stairs_06-10-2021_11...  \n",
       "2        s1870467   rob  Respeck_s1870467_Climbing stairs_06-10-2021_11...  \n",
       "3        s1870467   rob  Respeck_s1870467_Climbing stairs_06-10-2021_11...  \n",
       "4        s1870467   rob  Respeck_s1870467_Climbing stairs_06-10-2021_11...  \n",
       "...           ...   ...                                                ...  \n",
       "492671   s2211228   NaN  Respeck_s2211228_Sitting bent backward_29-09-2...  \n",
       "492672   s2211228   NaN  Respeck_s2211228_Sitting bent backward_29-09-2...  \n",
       "492673   s2211228   NaN  Respeck_s2211228_Sitting bent backward_29-09-2...  \n",
       "492674   s2211228   NaN  Respeck_s2211228_Sitting bent backward_29-09-2...  \n",
       "492675   s2211228   NaN  Respeck_s2211228_Sitting bent backward_29-09-2...  \n",
       "\n",
       "[492676 rows x 13 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_of_interest_initial = ['accel_x', 'accel_y', 'accel_z', 'gyro_x', 'gyro_y', 'gyro_z','subject_id','activity_code', 'activity_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>accel_x</th>\n",
       "      <th>accel_y</th>\n",
       "      <th>accel_z</th>\n",
       "      <th>gyro_x</th>\n",
       "      <th>gyro_y</th>\n",
       "      <th>gyro_z</th>\n",
       "      <th>sensor_type</th>\n",
       "      <th>activity_type</th>\n",
       "      <th>activity_code</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>notes</th>\n",
       "      <th>recording_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.633516e+12</td>\n",
       "      <td>0.261475</td>\n",
       "      <td>-1.116516</td>\n",
       "      <td>-0.502991</td>\n",
       "      <td>-0.812500</td>\n",
       "      <td>12.312500</td>\n",
       "      <td>19.500000</td>\n",
       "      <td>Respeck</td>\n",
       "      <td>Climbing stairs</td>\n",
       "      <td>12.0</td>\n",
       "      <td>s1870467</td>\n",
       "      <td>rob</td>\n",
       "      <td>Respeck_s1870467_Climbing stairs_06-10-2021_11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.633516e+12</td>\n",
       "      <td>-0.177734</td>\n",
       "      <td>-0.636292</td>\n",
       "      <td>-0.477600</td>\n",
       "      <td>-5.265625</td>\n",
       "      <td>2.953125</td>\n",
       "      <td>-9.281250</td>\n",
       "      <td>Respeck</td>\n",
       "      <td>Climbing stairs</td>\n",
       "      <td>12.0</td>\n",
       "      <td>s1870467</td>\n",
       "      <td>rob</td>\n",
       "      <td>Respeck_s1870467_Climbing stairs_06-10-2021_11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.633516e+12</td>\n",
       "      <td>0.351562</td>\n",
       "      <td>-0.879456</td>\n",
       "      <td>-0.335754</td>\n",
       "      <td>5.671875</td>\n",
       "      <td>24.656250</td>\n",
       "      <td>-10.562500</td>\n",
       "      <td>Respeck</td>\n",
       "      <td>Climbing stairs</td>\n",
       "      <td>12.0</td>\n",
       "      <td>s1870467</td>\n",
       "      <td>rob</td>\n",
       "      <td>Respeck_s1870467_Climbing stairs_06-10-2021_11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.633516e+12</td>\n",
       "      <td>0.298584</td>\n",
       "      <td>-1.253479</td>\n",
       "      <td>-0.299622</td>\n",
       "      <td>-3.609375</td>\n",
       "      <td>2.687500</td>\n",
       "      <td>5.890625</td>\n",
       "      <td>Respeck</td>\n",
       "      <td>Climbing stairs</td>\n",
       "      <td>12.0</td>\n",
       "      <td>s1870467</td>\n",
       "      <td>rob</td>\n",
       "      <td>Respeck_s1870467_Climbing stairs_06-10-2021_11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.633516e+12</td>\n",
       "      <td>0.056152</td>\n",
       "      <td>-0.777405</td>\n",
       "      <td>-0.210754</td>\n",
       "      <td>-14.328125</td>\n",
       "      <td>5.421875</td>\n",
       "      <td>8.140625</td>\n",
       "      <td>Respeck</td>\n",
       "      <td>Climbing stairs</td>\n",
       "      <td>12.0</td>\n",
       "      <td>s1870467</td>\n",
       "      <td>rob</td>\n",
       "      <td>Respeck_s1870467_Climbing stairs_06-10-2021_11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492670</th>\n",
       "      <td>1.632930e+12</td>\n",
       "      <td>-0.376465</td>\n",
       "      <td>-0.746399</td>\n",
       "      <td>0.693543</td>\n",
       "      <td>-0.171875</td>\n",
       "      <td>-0.953125</td>\n",
       "      <td>1.281250</td>\n",
       "      <td>Respeck</td>\n",
       "      <td>Sitting bent backward</td>\n",
       "      <td>5.0</td>\n",
       "      <td>s2211228</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Respeck_s2211228_Sitting bent backward_29-09-2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492671</th>\n",
       "      <td>1.632930e+12</td>\n",
       "      <td>-0.345703</td>\n",
       "      <td>-0.749329</td>\n",
       "      <td>0.677429</td>\n",
       "      <td>-0.671875</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>-1.015625</td>\n",
       "      <td>Respeck</td>\n",
       "      <td>Sitting bent backward</td>\n",
       "      <td>5.0</td>\n",
       "      <td>s2211228</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Respeck_s2211228_Sitting bent backward_29-09-2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492672</th>\n",
       "      <td>1.632930e+12</td>\n",
       "      <td>-0.368408</td>\n",
       "      <td>-0.745422</td>\n",
       "      <td>0.690857</td>\n",
       "      <td>-0.218750</td>\n",
       "      <td>-0.640625</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>Respeck</td>\n",
       "      <td>Sitting bent backward</td>\n",
       "      <td>5.0</td>\n",
       "      <td>s2211228</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Respeck_s2211228_Sitting bent backward_29-09-2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492673</th>\n",
       "      <td>1.632930e+12</td>\n",
       "      <td>-0.363281</td>\n",
       "      <td>-0.747375</td>\n",
       "      <td>0.689148</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.796875</td>\n",
       "      <td>-0.046875</td>\n",
       "      <td>Respeck</td>\n",
       "      <td>Sitting bent backward</td>\n",
       "      <td>5.0</td>\n",
       "      <td>s2211228</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Respeck_s2211228_Sitting bent backward_29-09-2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492674</th>\n",
       "      <td>1.632930e+12</td>\n",
       "      <td>-0.369873</td>\n",
       "      <td>-0.734436</td>\n",
       "      <td>0.684753</td>\n",
       "      <td>-0.140625</td>\n",
       "      <td>-0.531250</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>Respeck</td>\n",
       "      <td>Sitting bent backward</td>\n",
       "      <td>5.0</td>\n",
       "      <td>s2211228</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Respeck_s2211228_Sitting bent backward_29-09-2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>492675 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           timestamp   accel_x   accel_y   accel_z     gyro_x     gyro_y  \\\n",
       "0       1.633516e+12  0.261475 -1.116516 -0.502991  -0.812500  12.312500   \n",
       "1       1.633516e+12 -0.177734 -0.636292 -0.477600  -5.265625   2.953125   \n",
       "2       1.633516e+12  0.351562 -0.879456 -0.335754   5.671875  24.656250   \n",
       "3       1.633516e+12  0.298584 -1.253479 -0.299622  -3.609375   2.687500   \n",
       "4       1.633516e+12  0.056152 -0.777405 -0.210754 -14.328125   5.421875   \n",
       "...              ...       ...       ...       ...        ...        ...   \n",
       "492670  1.632930e+12 -0.376465 -0.746399  0.693543  -0.171875  -0.953125   \n",
       "492671  1.632930e+12 -0.345703 -0.749329  0.677429  -0.671875  -0.187500   \n",
       "492672  1.632930e+12 -0.368408 -0.745422  0.690857  -0.218750  -0.640625   \n",
       "492673  1.632930e+12 -0.363281 -0.747375  0.689148   0.078125   0.796875   \n",
       "492674  1.632930e+12 -0.369873 -0.734436  0.684753  -0.140625  -0.531250   \n",
       "\n",
       "           gyro_z sensor_type          activity_type  activity_code  \\\n",
       "0       19.500000     Respeck        Climbing stairs           12.0   \n",
       "1       -9.281250     Respeck        Climbing stairs           12.0   \n",
       "2      -10.562500     Respeck        Climbing stairs           12.0   \n",
       "3        5.890625     Respeck        Climbing stairs           12.0   \n",
       "4        8.140625     Respeck        Climbing stairs           12.0   \n",
       "...           ...         ...                    ...            ...   \n",
       "492670   1.281250     Respeck  Sitting bent backward            5.0   \n",
       "492671  -1.015625     Respeck  Sitting bent backward            5.0   \n",
       "492672   0.375000     Respeck  Sitting bent backward            5.0   \n",
       "492673  -0.046875     Respeck  Sitting bent backward            5.0   \n",
       "492674   0.687500     Respeck  Sitting bent backward            5.0   \n",
       "\n",
       "       subject_id notes                                       recording_id  \n",
       "0        s1870467   rob  Respeck_s1870467_Climbing stairs_06-10-2021_11...  \n",
       "1        s1870467   rob  Respeck_s1870467_Climbing stairs_06-10-2021_11...  \n",
       "2        s1870467   rob  Respeck_s1870467_Climbing stairs_06-10-2021_11...  \n",
       "3        s1870467   rob  Respeck_s1870467_Climbing stairs_06-10-2021_11...  \n",
       "4        s1870467   rob  Respeck_s1870467_Climbing stairs_06-10-2021_11...  \n",
       "...           ...   ...                                                ...  \n",
       "492670   s2211228   NaN  Respeck_s2211228_Sitting bent backward_29-09-2...  \n",
       "492671   s2211228   NaN  Respeck_s2211228_Sitting bent backward_29-09-2...  \n",
       "492672   s2211228   NaN  Respeck_s2211228_Sitting bent backward_29-09-2...  \n",
       "492673   s2211228   NaN  Respeck_s2211228_Sitting bent backward_29-09-2...  \n",
       "492674   s2211228   NaN  Respeck_s2211228_Sitting bent backward_29-09-2...  \n",
       "\n",
       "[492675 rows x 13 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_df = base_df.dropna(subset=columns_of_interest_initial).reset_index(drop=True)\n",
    "base_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can get a list of all recording ids, activities, sensor types and anything else you might need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data was collected using the sensors: ['Respeck']\n",
      "The data was collected for the activities: ['Climbing stairs' 'Standing' 'Movement' 'Lying down on stomach'\n",
      " 'Sitting bent backward' 'Lying down left' 'Lying down on back'\n",
      " 'Descending stairs' 'Sitting bent forward' 'Walking at normal speed'\n",
      " 'Running' 'Lying down right' 'Desk work' 'Sitting' 'Falling on knees'\n",
      " 'Falling on the back' 'Falling on the right' 'Falling on the left']\n",
      "The number of unique recordings is: 876\n",
      "The subject IDs in the recordings are: 46\n"
     ]
    }
   ],
   "source": [
    "print(f\"The data was collected using the sensors: {base_df.sensor_type.unique()}\")\n",
    "print(f\"The data was collected for the activities: {base_df.activity_type.unique()}\")\n",
    "print(f\"The number of unique recordings is: {len(base_df.recording_id.unique())}\")\n",
    "print(f\"The subject IDs in the recordings are: {len(base_df.subject_id.unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "X_with_subject_id = base_df[columns_of_interest_initial]\n",
    "y_with_subject_id = base_df['activity_code']\n",
    "\n",
    "\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "#                                                     test_size=0.2, train_size=0.8)\n",
    "\n",
    "#Split by subject id\n",
    "gs = GroupShuffleSplit(n_splits=2, test_size=0.2, random_state=1)\n",
    "train_ix, test_ix = next(gs.split(X_with_subject_id, y_with_subject_id, groups=X_with_subject_id.subject_id))\n",
    "\n",
    "\n",
    "X_train_df = X_with_subject_id.loc[train_ix]\n",
    "X_test_df = X_with_subject_id.loc[test_ix]\n",
    "\n",
    "\n",
    "# #Convert categorical variable (labels) into dummy/indicator variables (one-hot encoding)\n",
    "# y_train = np.asarray(pd.get_dummies(y_train), dtype=np.float32)\n",
    "# y_test = np.asarray(pd.get_dummies(y_test), dtype=np.float32)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The subject IDs in the recordings are: 46\n",
      "The subject IDs in the training set are: 36\n",
      "The subject IDs in the test set are: 10\n"
     ]
    }
   ],
   "source": [
    "#Check how many subject_ids are in each of training and test set\n",
    "print(f\"The subject IDs in the recordings are: {len(base_df.subject_id.unique())}\")\n",
    "print(f\"The subject IDs in the training set are: {len(X_train_df.subject_id.unique())}\")\n",
    "print(f\"The subject IDs in the test set are: {len(X_test_df.subject_id.unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can of course change the clean data folder to where you keep all the PDIoT data and you should be seeing a lot more subject IDs, unique recordings and activity types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting data into sliding windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sliding window approach is one of the most efficient ways to process Human Activity Recognition data. We saw in the last notebook that sensor data comes in the form of Time Series. One single datapoint is not enough to represent an activity, we need a larger snapshot of the signal for that. The image below shows how a sliding window achieves that. \n",
    "\n",
    "![sliding windows](../Images/sliding_windows_complete.png \"Sliding Windows\")\n",
    "\n",
    "The windows can have some amount of overlap, as shown in the picture, or they can have no overlap at all in which case they would be side-by-side.\n",
    "\n",
    "Each window can now be treated as an input datapoint to whichever model you choose to train. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Watch out for separate recordings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to make sure that when you split your data into sliding windows you don't accidentally include two separate recordings in the same window. This would cause the signal from the first recording to suddenly \"jump\" to an unrelated value from the second recording. \n",
    "\n",
    "For this, you will have to first split up your dataset by recording (this is where the recording ID comes in), then you have to split each recording into sliding windows. At the end you can aggregate all of your resulting sliding windows in a large dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From sliding windows to datapoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each sliding window needs to be further processed in order to represent an appropriate input datapoint. The preprocessing methods might differ depending on which type of model you choose to use. \n",
    "\n",
    "You can also do further processing on the signal types and axes, for example smooth the signal, apply axis fusion, eliminate noise etc. \n",
    "\n",
    "Here we will discuss simple examples without any preprocessing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction with tsfresh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the index represents the window ID, and each row in the DataFrame represents a multi-dimensional datapoint which we can use as input to the RFC.\n",
    "\n",
    "You can use the window ID to refer back to the initial dataframe and get the class (activity type) for each window."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now process both recordings so that we have two classes for our classifier. For your own work you should only aim to perform classification on data from one sensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def group_into_sliding_windows(df,window_size=50,step_size=25):\n",
    "\n",
    "    window_number = 0 # start a counter at 0 to keep track of the window number\n",
    "\n",
    "    all_overlapping_windows = []\n",
    "\n",
    "\n",
    "    for rid, group in df.groupby(\"activity_code\"):\n",
    "        print(f\"Processing rid = {rid}\")\n",
    "        large_enough_windows = [window for window in group.rolling(window=window_size, min_periods=window_size) if len(window) == window_size]\n",
    "\n",
    "\n",
    "\n",
    "        overlapping_windows = large_enough_windows[::step_size] \n",
    "        # then we will append a window ID to each window\n",
    "        for window in overlapping_windows:\n",
    "            window.loc[:, 'window_id'] = window_number\n",
    "            window_number += 1\n",
    "\n",
    "\n",
    "        all_overlapping_windows.append(pd.concat(overlapping_windows).reset_index(drop=True))\n",
    "        \n",
    "    final_sliding_windows = pd.concat(all_overlapping_windows).reset_index(drop=True)\n",
    "    \n",
    "    #Test to see if there are any elements that have NaN\n",
    "    is_NaN = final_sliding_windows.isnull()\n",
    "    row_has_NaN = is_NaN.any(axis=1)\n",
    "    rows_with_NaN = final_sliding_windows[row_has_NaN]\n",
    "\n",
    "    print(rows_with_NaN)\n",
    "    return final_sliding_windows.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing rid = 0.0\n",
      "Processing rid = 1.0\n",
      "Processing rid = 2.0\n",
      "Processing rid = 4.0\n",
      "Processing rid = 5.0\n",
      "Processing rid = 6.0\n",
      "Processing rid = 7.0\n",
      "Processing rid = 8.0\n",
      "Processing rid = 9.0\n",
      "Processing rid = 11.0\n",
      "Processing rid = 12.0\n",
      "Processing rid = 13.0\n",
      "Processing rid = 31.0\n",
      "Processing rid = 45.0\n",
      "Processing rid = 46.0\n",
      "Processing rid = 47.0\n",
      "Processing rid = 48.0\n",
      "Processing rid = 100.0\n",
      "Empty DataFrame\n",
      "Columns: [accel_x, accel_y, accel_z, gyro_x, gyro_y, gyro_z, subject_id, activity_code, activity_type, window_id]\n",
      "Index: []\n",
      "Processing rid = 0.0\n",
      "Processing rid = 1.0\n",
      "Processing rid = 2.0\n",
      "Processing rid = 4.0\n",
      "Processing rid = 5.0\n",
      "Processing rid = 6.0\n",
      "Processing rid = 7.0\n",
      "Processing rid = 8.0\n",
      "Processing rid = 9.0\n",
      "Processing rid = 11.0\n",
      "Processing rid = 12.0\n",
      "Processing rid = 13.0\n",
      "Processing rid = 31.0\n",
      "Processing rid = 45.0\n",
      "Processing rid = 46.0\n",
      "Processing rid = 47.0\n",
      "Processing rid = 48.0\n",
      "Processing rid = 100.0\n",
      "Empty DataFrame\n",
      "Columns: [accel_x, accel_y, accel_z, gyro_x, gyro_y, gyro_z, subject_id, activity_code, activity_type, window_id]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "window_size = 50 # 50 datapoints for the window size, which, at 25Hz, means 2 seconds\n",
    "step_size = 25 # this is 50% overlap\n",
    "\n",
    "X_train_sliding_windows = group_into_sliding_windows(X_train_df,window_size,step_size)\n",
    "X_test_sliding_windows = group_into_sliding_windows(X_test_df,window_size,step_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to assign a number to each categorical class for the model. It is mainly up to you how you categorise your classes. In this example, we will use the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Climbing stairs': 0, 'Descending stairs': 1, 'Desk work': 2, 'Falling on knees': 3, 'Falling on the back': 4, 'Falling on the left': 5, 'Falling on the right': 6, 'Lying down left': 7, 'Lying down on back': 8, 'Lying down on stomach': 9, 'Lying down right': 10, 'Movement': 11, 'Running': 12, 'Sitting': 13, 'Sitting bent backward': 14, 'Sitting bent forward': 15, 'Standing': 16, 'Walking at normal speed': 17}\n"
     ]
    }
   ],
   "source": [
    "class_labels = {}\n",
    "label_to_activity = {}\n",
    "activities = sorted(base_df.activity_type.unique())\n",
    "\n",
    "for idx, activity in enumerate(activities):\n",
    "    class_labels[activity] = idx\n",
    "    label_to_activity[idx] = activity\n",
    "\n",
    "print(class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# window_id_class_labels = final_sliding_windows.groupby(\"window_id\")[['activity_type']].agg(np.min).replace(class_labels)\n",
    "# window_id_class_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leave-one-subject-out cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the other reason our previous classifier functions so well is that each person performs activities in a very consistent manner. If a subject's data is both in the training set and the test set, it provides the model with an unfair advantage. Moreover, the results that your model will output will be falsely high. When you test your model on a completely new, unseen subject, your accuracy will drop considerably. \n",
    "\n",
    "This is why when training a HAR model you will want to do a special kind of cross-validation: Leave-One-Subject-Out (LOSOXV), where we leave one (or more) subject(s) in the testing set at each iteration.\n",
    "\n",
    "![losoxv](../Images/LOOCV.png \"losoxv\")\n",
    "\n",
    "This ensures that the results we get from our classifier are consistent to what we would get in real life, if we were to test the model on a new user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a simple CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some different preprocessing techniques you can apply when the resulting \"datapoint\" is an input to a convolutional neural network. \n",
    "\n",
    "You can use two types of convolutional layers:\n",
    "* 1D Conv Layers - which will work on 1D data, for example a single axis from one single sensor (accel, gyro or mag)\n",
    "* 2D Conv Layers - suitable if the input data is in the form of an image, for example\n",
    "\n",
    "We will be demonstrating how to build a simple 1D CNN using 6 channels: the 3 axes of the accelerometer and the 3 axes of the gyroscope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "filters = 64\n",
    "kernel_size = 3\n",
    "n_features = 6\n",
    "activation='relu'\n",
    "n_classes = len(class_labels)\n",
    "print(n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_12 (Conv1D)           (None, 48, 64)            1216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 48, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 48, 64)            0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 48, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 46, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 46, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 46, 64)            0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 46, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 44, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 44, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 44, 64)            0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 44, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 2816)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 100)               281700    \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 18)                1818      \n",
      "=================================================================\n",
      "Total params: 310,206\n",
      "Trainable params: 309,822\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv1D(filters=filters, kernel_size=kernel_size, activation='linear', \n",
    "                 input_shape=(window_size, n_features)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(activation))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv1D(filters=filters, kernel_size=kernel_size, activation='linear'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(activation))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "model.add(Conv1D(filters=filters, kernel_size=kernel_size, activation='linear'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(activation))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-generating the data in the appropriate format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to do a bit more work with our data to bring it into a format fit for training a CNN. \n",
    "\n",
    "A CNN will take multi-dimensional arrays as input. We have already specified that the input shape is (window_size, n_features), i.e. (50, 6). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that we generated sliding windows before. Now we just need to take the raw values from each window and create a training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['accel_x', 'accel_y', 'accel_z', 'gyro_x', 'gyro_y', 'gyro_z']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Subject id is of interest to us as we will split our dataset by subject_id\n",
    "columns_of_interest_training = ['accel_x', 'accel_y', 'accel_z', 'gyro_x', 'gyro_y', 'gyro_z']\n",
    "columns_of_interest_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regenerate_data_from_sliding_windows(final_sliding_windows):\n",
    "    X= []\n",
    "    y= []\n",
    "    for window_id, group in final_sliding_windows.groupby('window_id'):\n",
    "        \n",
    "    #     print(f\"window_id = {window_id}\")\n",
    "\n",
    "        shape = group[columns_of_interest_training].values.shape\n",
    "    #     print(f\"shape = {shape}\")\n",
    "\n",
    "        X.append(group[columns_of_interest_training].values)\n",
    "        y.append(class_labels[group[\"activity_type\"].values[0]])\n",
    "    \n",
    "    return X,y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_regenerated, y_train_regenerated = regenerate_data_from_sliding_windows(X_train_sliding_windows)\n",
    "X_test_regenerated, y_test_regenerated = regenerate_data_from_sliding_windows(X_test_sliding_windows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do a similar test/train split for demonstration purposes. Remember that you will have to split your data by subjects, not radomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape = (15416, 50, 6)\n",
      "y_train shape = (15416, 18)\n",
      "X_test shape = (4240, 50, 6)\n",
      "y_test shape = (4240, 18)\n"
     ]
    }
   ],
   "source": [
    "# #Convert categorical variable (labels) into dummy/indicator variables (one-hot encoding)\n",
    "y_train = np.asarray(pd.get_dummies(y_train_regenerated), dtype=np.float32)\n",
    "y_test = np.asarray(pd.get_dummies(y_test_regenerated), dtype=np.float32)\n",
    "\n",
    "X_train = np.asarray(X_train_regenerated)\n",
    "print(f\"X_train shape = {X_train.shape}\")\n",
    "print(f\"y_train shape = {y_train.shape}\")\n",
    "X_test = np.asarray(X_test_regenerated)\n",
    "\n",
    "print(f\"X_test shape = {X_test.shape}\")\n",
    "print(f\"y_test shape = {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_21 (Conv1D)           (None, 48, 64)            1216      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 16, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 14, 128)           24704     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 4, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 2, 256)            98560     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 18)                1170      \n",
      "=================================================================\n",
      "Total params: 166,802\n",
      "Trainable params: 166,802\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras as keras\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(keras.layers.Input(shape=(window_size, n_features)))\n",
    "\n",
    "model.add(keras.layers.Conv1D(64, kernel_size=kernel_size, activation=\"relu\"))\n",
    "model.add(keras.layers.MaxPooling1D(pool_size=3))\n",
    "\n",
    "model.add(keras.layers.Conv1D(128, kernel_size=kernel_size, activation=\"relu\"))\n",
    "model.add(keras.layers.MaxPooling1D(pool_size=3))\n",
    "\n",
    "model.add(keras.layers.Conv1D(256, kernel_size=kernel_size, activation=\"relu\"))\n",
    "model.add(keras.layers.GlobalMaxPooling1D())\n",
    "\n",
    "model.add(keras.layers.Dense(128, activation=\"relu\"))\n",
    "\n",
    "model.add(keras.layers.Dense(64, activation=\"relu\"))\n",
    "\n",
    "model.add(keras.layers.Dense(n_classes, activation='softmax'))\n",
    "\n",
    "# print model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to start the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)\n",
    "model.compile(\n",
    "    optimizer=optimizers.SGD(lr=0.01, momentum=0.9, decay=1e-6, nesterov=True),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15416 samples, validate on 4240 samples\n",
      "Epoch 1/100\n",
      "15416/15416 [==============================] - 12s 771us/sample - loss: 2.2982 - accuracy: 0.2752 - val_loss: 2.1664 - val_accuracy: 0.3163\n",
      "Epoch 2/100\n",
      "15416/15416 [==============================] - 8s 499us/sample - loss: 1.4369 - accuracy: 0.5215 - val_loss: 1.2376 - val_accuracy: 0.6132\n",
      "Epoch 3/100\n",
      "15416/15416 [==============================] - 8s 523us/sample - loss: 0.9592 - accuracy: 0.6648 - val_loss: 0.9896 - val_accuracy: 0.6507\n",
      "Epoch 4/100\n",
      "15416/15416 [==============================] - 8s 542us/sample - loss: 0.7703 - accuracy: 0.7128 - val_loss: 1.2622 - val_accuracy: 0.6394\n",
      "Epoch 5/100\n",
      "15416/15416 [==============================] - 8s 536us/sample - loss: 0.6800 - accuracy: 0.7409 - val_loss: 1.1147 - val_accuracy: 0.6696\n",
      "Epoch 6/100\n",
      "15416/15416 [==============================] - 9s 603us/sample - loss: 0.6155 - accuracy: 0.7689 - val_loss: 1.1845 - val_accuracy: 0.6660\n",
      "Epoch 7/100\n",
      "15416/15416 [==============================] - 8s 549us/sample - loss: 0.5582 - accuracy: 0.7881 - val_loss: 1.2136 - val_accuracy: 0.6642\n",
      "Epoch 8/100\n",
      "15416/15416 [==============================] - 9s 568us/sample - loss: 0.4995 - accuracy: 0.8112 - val_loss: 1.3337 - val_accuracy: 0.6460\n",
      "Epoch 9/100\n",
      "15416/15416 [==============================] - 9s 588us/sample - loss: 0.4549 - accuracy: 0.8242 - val_loss: 1.3798 - val_accuracy: 0.6467\n",
      "Epoch 10/100\n",
      "15416/15416 [==============================] - 9s 553us/sample - loss: 0.4168 - accuracy: 0.8401 - val_loss: 1.6393 - val_accuracy: 0.6486\n",
      "Epoch 11/100\n",
      "15416/15416 [==============================] - 9s 570us/sample - loss: 0.3854 - accuracy: 0.8532 - val_loss: 1.4522 - val_accuracy: 0.6599\n",
      "Epoch 12/100\n",
      "15416/15416 [==============================] - 9s 563us/sample - loss: 0.3779 - accuracy: 0.8622 - val_loss: 1.7305 - val_accuracy: 0.6087\n",
      "Epoch 13/100\n",
      "13056/15416 [========================>.....] - ETA: 1s - loss: 0.3541 - accuracy: 0.8686"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-100-42548ee86092>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m model.fit(X_train, y_train,\n\u001b[1;32m      2\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m )\n",
      "\u001b[0;32m~/miniconda3/envs/pdiot/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/miniconda3/envs/pdiot/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pdiot/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pdiot/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pdiot/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pdiot/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    485\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pdiot/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pdiot/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1141\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pdiot/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pdiot/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/miniconda3/envs/pdiot/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train,\n",
    "        batch_size=128, epochs=100, callbacks=[callback],\n",
    "        validation_data=(X_test, y_test),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now view the accuracy of our model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats\n",
    "y_pred_ohe = model.predict(X_test)\n",
    "y_pred_labels = np.argmax(y_pred_ohe, axis=1)\n",
    "y_true_labels = np.argmax(y_test, axis=1)\n",
    "print(y_pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"*\" * 80)\n",
    "print(\"Classification report\")\n",
    "print(\"*\" * 80)\n",
    "print(classification_report(y_true_labels, y_pred_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are now ready to start developing your own models for HAR. There are numerous tutorials online which you can follow to build models like LSTMs, CNNs, RFCs and many others. \n",
    "\n",
    "You have a wide choice of ways to solve this classification model. Here are a few things to think about:\n",
    "\n",
    "* What type of preprocessing do you want to apply to your data? Examples include:\n",
    "    * smoothing the sensor axes\n",
    "    * performing axis fusion\n",
    "    * extracting scalograms from the signal\n",
    "    * manually extracting features from the signal\n",
    "    * choosing to leave out certain axes\n",
    "\n",
    "* What type of model do you want to train?\n",
    "    * simple ML model\n",
    "    * deep learning model\n",
    "    \n",
    "* Do you want a hierarchical model or a flat model?\n",
    "    * hierarchical models means you don't have to train the same type of model for each activity\n",
    "    * a flat model might be faster to train and apply in real time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_directory = './models/'\n",
    "current_model_path = models_directory + 'CNN_model_HAR_v2/'\n",
    "\n",
    "tflite_model_filename = 'CNN_HAR_v2.tflite'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/CNN_model_HARv_v1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/CNN_model_HARv_v1/assets\n"
     ]
    }
   ],
   "source": [
    "#Save original model first. We will use the SavedModel to convert it to TFLite as recommended by the Tensorflow documentation.\n",
    "tf.saved_model.save(model, current_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert SavedModel to TFLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the model\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(current_model_path) # path to the SavedModel directory\n",
    "tflite_model = converter.convert()\n",
    "tflite_model_filename = 'CNN_HAR_v1.tflite'\n",
    "# Save the model.\n",
    "with open(current_model_path+tflite_model_filename, 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Test on Python - Tensorflow Lite\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape\n",
      "[ 1 50  6]\n",
      "See Test Shape\n",
      "(1, 50, 6)\n",
      "Current output results\n",
      "[6.2270577e-05 2.4624990e-06 1.5130171e-01 3.9093671e-04 5.0324547e-06\n",
      " 7.4569823e-04 3.8591963e-05 1.3974484e-03 2.7534372e-06 2.3628731e-07\n",
      " 1.2048521e-04 8.6326441e-03 9.0560906e-07 2.8892693e-01 3.9909825e-02\n",
      " 4.7166407e-01 3.6739558e-02 5.8311980e-05]\n",
      "18\n",
      "Which is the most confident?\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "# Load TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=current_model_path+tflite_model_filename)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Test model on random input data.\n",
    "input_shape = input_details[0]['shape']\n",
    "print(\"Input Shape\")\n",
    "print(input_shape)\n",
    "test = X_test.astype(np.float32)\n",
    "#Test data to feed as parameter\n",
    "test_part = test[2:3]\n",
    "print(\"See Test Shape\")\n",
    "print(test_part.shape)\n",
    "\n",
    "interpreter.set_tensor(input_details[0]['index'], test_part)\n",
    "\n",
    "interpreter.invoke()\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "output_results = np.squeeze(output_data)\n",
    "\n",
    "print(\"Current output results\")\n",
    "print(output_results)\n",
    "print(len(output_results))\n",
    "\n",
    "print(\"Which is the most confident?\")\n",
    "max_index = np.argmax(output_results, axis=0)\n",
    "\n",
    "print(max_index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
