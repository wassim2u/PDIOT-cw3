{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Introduction to Human Activity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will explore some common ways of preprocessing human activity recognition data.\n",
    "\n",
    "Using the example data we will learn:\n",
    "* how to merge multiple files into one large DataFrame\n",
    "* how to split data into sliding windows\n",
    "* how to quickly extract features from a window\n",
    "* how to set the number of classes considered for classification\n",
    "* how to build a simple Random Forest Classifier and train it on HAR data\n",
    "* how to build a simple CNN and train it on HAR data \n",
    "\n",
    "Bear in mind that the sample data offered is not cleaned or high quality. You should not use it in your own experiments but it is useful for this tutorial.\n",
    "\n",
    "You will need the following packages: \n",
    "* tsfresh\n",
    "* scikit-learn\n",
    "* tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current Model Note for cw3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tsfresh\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# keras goodies\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Conv1D, Dropout, MaxPooling1D, BatchNormalization\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import metrics as kmetrics\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading multiple files into one large DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage you should only be working with clean data, saved in the format required for Coursework 1. An example of such data can be found in the Data/Clean/ folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s1870467\n",
      "s1870697\n",
      "s2171931\n",
      "s2254050\n",
      "s1727780\n",
      "s2125423\n",
      ".DS_Store\n",
      ".DS_Store  is not a folder\n",
      "s1757177\n",
      "s1702583\n",
      "s1824891\n",
      "s1826390\n",
      "s1865890\n",
      "s1823274\n",
      "s1865457\n",
      "s1800883\n",
      "s1718069\n",
      "s1706154\n",
      "s1864705\n",
      "s1817455\n",
      "s2211162\n",
      "s1761322\n",
      "s1970333\n",
      "s2192970\n",
      "s2181154\n",
      "s1711661\n",
      "s2250677\n",
      "s1893474\n",
      "s2119637\n",
      "s1721256\n",
      "s1704145\n",
      "s1894401\n",
      "s1541031\n",
      "s1843072\n",
      "s1893835\n",
      "README.md\n",
      "README.md  is not a folder\n",
      "s2173036\n",
      "s2212045\n",
      "s1813106\n",
      "s1891214\n",
      "s1724279\n",
      "s1822958\n",
      "s1841064\n",
      "s2171825\n",
      "s1714206\n",
      "s1801931\n",
      "s1842093\n",
      "s1724067\n",
      "s1732873\n",
      "s1817972\n",
      "s1850642\n",
      "s2211228\n"
     ]
    }
   ],
   "source": [
    "base_df = pd.DataFrame()\n",
    "\n",
    "clean_data_folder = \"../pdiot-data/2021\"\n",
    "\n",
    "for sUNN_folder in os.listdir(clean_data_folder):\n",
    "    print(sUNN_folder)\n",
    "    try:\n",
    "        for filename in os.listdir(clean_data_folder+\"/\"+sUNN_folder):\n",
    "            \n",
    "            full_path = f\"{clean_data_folder}/{sUNN_folder}/{filename}\"\n",
    "#             print(full_path)\n",
    "\n",
    "            # load data into a DataFrame\n",
    "            new_df = pd.read_csv(full_path)\n",
    "\n",
    "            # merge into the base DataFrame\n",
    "            base_df = pd.concat([base_df, new_df])\n",
    "    except:\n",
    "        print(sUNN_folder, \" is not a folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reset the index of the base DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>accel_x</th>\n",
       "      <th>accel_y</th>\n",
       "      <th>accel_z</th>\n",
       "      <th>gyro_x</th>\n",
       "      <th>gyro_y</th>\n",
       "      <th>gyro_z</th>\n",
       "      <th>sensor_type</th>\n",
       "      <th>activity_type</th>\n",
       "      <th>activity_code</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>recording_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.633516e+12</td>\n",
       "      <td>0.261475</td>\n",
       "      <td>-1.116516</td>\n",
       "      <td>-0.502991</td>\n",
       "      <td>-0.812500</td>\n",
       "      <td>12.312500</td>\n",
       "      <td>19.500000</td>\n",
       "      <td>Respeck</td>\n",
       "      <td>Climbing stairs</td>\n",
       "      <td>12</td>\n",
       "      <td>s1870467</td>\n",
       "      <td>Respeck_s1870467_Climbing stairs_06-10-2021_11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.633516e+12</td>\n",
       "      <td>-0.177734</td>\n",
       "      <td>-0.636292</td>\n",
       "      <td>-0.477600</td>\n",
       "      <td>-5.265625</td>\n",
       "      <td>2.953125</td>\n",
       "      <td>-9.281250</td>\n",
       "      <td>Respeck</td>\n",
       "      <td>Climbing stairs</td>\n",
       "      <td>12</td>\n",
       "      <td>s1870467</td>\n",
       "      <td>Respeck_s1870467_Climbing stairs_06-10-2021_11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.633516e+12</td>\n",
       "      <td>0.351562</td>\n",
       "      <td>-0.879456</td>\n",
       "      <td>-0.335754</td>\n",
       "      <td>5.671875</td>\n",
       "      <td>24.656250</td>\n",
       "      <td>-10.562500</td>\n",
       "      <td>Respeck</td>\n",
       "      <td>Climbing stairs</td>\n",
       "      <td>12</td>\n",
       "      <td>s1870467</td>\n",
       "      <td>Respeck_s1870467_Climbing stairs_06-10-2021_11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.633516e+12</td>\n",
       "      <td>0.298584</td>\n",
       "      <td>-1.253479</td>\n",
       "      <td>-0.299622</td>\n",
       "      <td>-3.609375</td>\n",
       "      <td>2.687500</td>\n",
       "      <td>5.890625</td>\n",
       "      <td>Respeck</td>\n",
       "      <td>Climbing stairs</td>\n",
       "      <td>12</td>\n",
       "      <td>s1870467</td>\n",
       "      <td>Respeck_s1870467_Climbing stairs_06-10-2021_11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.633516e+12</td>\n",
       "      <td>0.056152</td>\n",
       "      <td>-0.777405</td>\n",
       "      <td>-0.210754</td>\n",
       "      <td>-14.328125</td>\n",
       "      <td>5.421875</td>\n",
       "      <td>8.140625</td>\n",
       "      <td>Respeck</td>\n",
       "      <td>Climbing stairs</td>\n",
       "      <td>12</td>\n",
       "      <td>s1870467</td>\n",
       "      <td>Respeck_s1870467_Climbing stairs_06-10-2021_11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673313</th>\n",
       "      <td>1.632930e+12</td>\n",
       "      <td>-0.376465</td>\n",
       "      <td>-0.746399</td>\n",
       "      <td>0.693542</td>\n",
       "      <td>-0.171875</td>\n",
       "      <td>-0.953125</td>\n",
       "      <td>1.281250</td>\n",
       "      <td>Respeck</td>\n",
       "      <td>Sitting bent backward</td>\n",
       "      <td>5</td>\n",
       "      <td>s2211228</td>\n",
       "      <td>Respeck_s2211228_Sitting bent backward_29-09-2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673314</th>\n",
       "      <td>1.632930e+12</td>\n",
       "      <td>-0.345703</td>\n",
       "      <td>-0.749329</td>\n",
       "      <td>0.677429</td>\n",
       "      <td>-0.671875</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>-1.015625</td>\n",
       "      <td>Respeck</td>\n",
       "      <td>Sitting bent backward</td>\n",
       "      <td>5</td>\n",
       "      <td>s2211228</td>\n",
       "      <td>Respeck_s2211228_Sitting bent backward_29-09-2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673315</th>\n",
       "      <td>1.632930e+12</td>\n",
       "      <td>-0.368408</td>\n",
       "      <td>-0.745422</td>\n",
       "      <td>0.690857</td>\n",
       "      <td>-0.218750</td>\n",
       "      <td>-0.640625</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>Respeck</td>\n",
       "      <td>Sitting bent backward</td>\n",
       "      <td>5</td>\n",
       "      <td>s2211228</td>\n",
       "      <td>Respeck_s2211228_Sitting bent backward_29-09-2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673316</th>\n",
       "      <td>1.632930e+12</td>\n",
       "      <td>-0.363281</td>\n",
       "      <td>-0.747375</td>\n",
       "      <td>0.689148</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.796875</td>\n",
       "      <td>-0.046875</td>\n",
       "      <td>Respeck</td>\n",
       "      <td>Sitting bent backward</td>\n",
       "      <td>5</td>\n",
       "      <td>s2211228</td>\n",
       "      <td>Respeck_s2211228_Sitting bent backward_29-09-2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673317</th>\n",
       "      <td>1.632930e+12</td>\n",
       "      <td>-0.369873</td>\n",
       "      <td>-0.734436</td>\n",
       "      <td>0.684753</td>\n",
       "      <td>-0.140625</td>\n",
       "      <td>-0.531250</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>Respeck</td>\n",
       "      <td>Sitting bent backward</td>\n",
       "      <td>5</td>\n",
       "      <td>s2211228</td>\n",
       "      <td>Respeck_s2211228_Sitting bent backward_29-09-2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>673318 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           timestamp   accel_x   accel_y   accel_z     gyro_x     gyro_y  \\\n",
       "0       1.633516e+12  0.261475 -1.116516 -0.502991  -0.812500  12.312500   \n",
       "1       1.633516e+12 -0.177734 -0.636292 -0.477600  -5.265625   2.953125   \n",
       "2       1.633516e+12  0.351562 -0.879456 -0.335754   5.671875  24.656250   \n",
       "3       1.633516e+12  0.298584 -1.253479 -0.299622  -3.609375   2.687500   \n",
       "4       1.633516e+12  0.056152 -0.777405 -0.210754 -14.328125   5.421875   \n",
       "...              ...       ...       ...       ...        ...        ...   \n",
       "673313  1.632930e+12 -0.376465 -0.746399  0.693542  -0.171875  -0.953125   \n",
       "673314  1.632930e+12 -0.345703 -0.749329  0.677429  -0.671875  -0.187500   \n",
       "673315  1.632930e+12 -0.368408 -0.745422  0.690857  -0.218750  -0.640625   \n",
       "673316  1.632930e+12 -0.363281 -0.747375  0.689148   0.078125   0.796875   \n",
       "673317  1.632930e+12 -0.369873 -0.734436  0.684753  -0.140625  -0.531250   \n",
       "\n",
       "           gyro_z sensor_type          activity_type  activity_code  \\\n",
       "0       19.500000     Respeck        Climbing stairs             12   \n",
       "1       -9.281250     Respeck        Climbing stairs             12   \n",
       "2      -10.562500     Respeck        Climbing stairs             12   \n",
       "3        5.890625     Respeck        Climbing stairs             12   \n",
       "4        8.140625     Respeck        Climbing stairs             12   \n",
       "...           ...         ...                    ...            ...   \n",
       "673313   1.281250     Respeck  Sitting bent backward              5   \n",
       "673314  -1.015625     Respeck  Sitting bent backward              5   \n",
       "673315   0.375000     Respeck  Sitting bent backward              5   \n",
       "673316  -0.046875     Respeck  Sitting bent backward              5   \n",
       "673317   0.687500     Respeck  Sitting bent backward              5   \n",
       "\n",
       "       subject_id                                       recording_id  \n",
       "0        s1870467  Respeck_s1870467_Climbing stairs_06-10-2021_11...  \n",
       "1        s1870467  Respeck_s1870467_Climbing stairs_06-10-2021_11...  \n",
       "2        s1870467  Respeck_s1870467_Climbing stairs_06-10-2021_11...  \n",
       "3        s1870467  Respeck_s1870467_Climbing stairs_06-10-2021_11...  \n",
       "4        s1870467  Respeck_s1870467_Climbing stairs_06-10-2021_11...  \n",
       "...           ...                                                ...  \n",
       "673313   s2211228  Respeck_s2211228_Sitting bent backward_29-09-2...  \n",
       "673314   s2211228  Respeck_s2211228_Sitting bent backward_29-09-2...  \n",
       "673315   s2211228  Respeck_s2211228_Sitting bent backward_29-09-2...  \n",
       "673316   s2211228  Respeck_s2211228_Sitting bent backward_29-09-2...  \n",
       "673317   s2211228  Respeck_s2211228_Sitting bent backward_29-09-2...  \n",
       "\n",
       "[673318 rows x 12 columns]"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use only respeck data. Also, remove columns that are added by mistake \n",
    "required_columns = [\"timestamp\",\"accel_x\",\"accel_y\",\"accel_z\",\"gyro_x\",\"gyro_y\",\"gyro_z\",\"sensor_type\",\"activity_type\", \"activity_code\", \"subject_id\",\"recording_id\"]\n",
    "base_df.reset_index(drop=True, inplace=True)\n",
    "base_df = base_df[required_columns]\n",
    "base_df = base_df[base_df.sensor_type==\"Respeck\"] \n",
    "base_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "base_df.dropna(inplace=True)\n",
    "base_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can get a list of all recording ids, activities, sensor types and anything else you might need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data was collected using the sensors: ['Respeck']\n",
      "The data was collected for the activities: ['Climbing stairs' 'Standing' 'Falling on the back' 'Movement'\n",
      " 'Lying down on stomach' 'Sitting bent backward' 'Lying down left'\n",
      " 'Lying down on back' 'Descending stairs' 'Sitting bent forward'\n",
      " 'Walking at normal speed' 'Running' 'Falling on the left'\n",
      " 'Lying down right' 'Falling on the right' 'Desk work' 'Falling on knees'\n",
      " 'Sitting']\n",
      "The number of unique recordings is: 3488\n",
      "The subject IDs in the recordings are: 48\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "print(f\"The data was collected using the sensors: {base_df.sensor_type.unique()}\")\n",
    "print(f\"The data was collected for the activities: {base_df.activity_type.unique()}\")\n",
    "print(f\"The number of unique recordings is: {len(base_df.recording_id.unique())}\")\n",
    "print(f\"The subject IDs in the recordings are: {len(base_df.subject_id.unique())}\")\n",
    "print(len(base_df.activity_type.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can of course change the clean data folder to where you keep all the PDIoT data and you should be seeing a lot more subject IDs, unique recordings and activity types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting data into sliding windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sliding window approach is one of the most efficient ways to process Human Activity Recognition data. We saw in the last notebook that sensor data comes in the form of Time Series. One single datapoint is not enough to represent an activity, we need a larger snapshot of the signal for that. The image below shows how a sliding window achieves that. \n",
    "\n",
    "![sliding windows](../Images/sliding_windows_complete.png \"Sliding Windows\")\n",
    "\n",
    "The windows can have some amount of overlap, as shown in the picture, or they can have no overlap at all in which case they would be side-by-side.\n",
    "\n",
    "Each window can now be treated as an input datapoint to whichever model you choose to train. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Watch out for separate recordings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to make sure that when you split your data into sliding windows you don't accidentally include two separate recordings in the same window. This would cause the signal from the first recording to suddenly \"jump\" to an unrelated value from the second recording. \n",
    "\n",
    "For this, you will have to first split up your dataset by recording (this is where the recording ID comes in), then you have to split each recording into sliding windows. At the end you can aggregate all of your resulting sliding windows in a large dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_of_interest = ['accel_x', 'accel_y', 'accel_z', 'gyro_x', 'gyro_y', 'gyro_z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 6)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(first_four_windows[0][columns_of_interest]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From sliding windows to datapoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each sliding window needs to be further processed in order to represent an appropriate input datapoint. The preprocessing methods might differ depending on which type of model you choose to use. \n",
    "\n",
    "You can also do further processing on the signal types and axes, for example smooth the signal, apply axis fusion, eliminate noise etc. \n",
    "\n",
    "Here we will discuss simple examples without any preprocessing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction with tsfresh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One quick and simple method to consider for HAR is a [Random Forest Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html). It can take a multi-dimensional datapoint as input and can output a classification. \n",
    "\n",
    "We can transform each window of data into one single, multidimensional datapoint by applying simple computations using the [tsfresh](https://tsfresh.readthedocs.io/en/latest/text/feature_extraction_settings.html) package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the index represents the window ID, and each row in the DataFrame represents a multi-dimensional datapoint which we can use as input to the RFC.\n",
    "\n",
    "You can use the window ID to refer back to the initial dataframe and get the class (activity type) for each window."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now process both recordings so that we have two classes for our classifier. For your own work you should only aim to perform classification on data from one sensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing rid = 0\n",
      "Processing rid = 1\n",
      "Processing rid = 2\n",
      "Processing rid = 4\n",
      "Processing rid = 5\n",
      "Processing rid = 6\n",
      "Processing rid = 7\n",
      "Processing rid = 8\n",
      "Processing rid = 9\n",
      "Processing rid = 11\n",
      "Processing rid = 12\n",
      "Processing rid = 13\n",
      "Processing rid = 31\n",
      "Processing rid = 45\n",
      "Processing rid = 46\n",
      "Processing rid = 47\n",
      "Processing rid = 48\n",
      "Processing rid = 100\n"
     ]
    }
   ],
   "source": [
    "window_size = 50 # 50 datapoints for the window size, which, at 25Hz, means 2 seconds\n",
    "step_size = 25 # this is 50% overlap\n",
    "\n",
    "window_number = 0 # start a counter at 0 to keep track of the window number\n",
    "\n",
    "all_overlapping_windows = []\n",
    "\n",
    "# copy_df = df\n",
    "# copy_df.reset_index(drop=True, inplace=True)\n",
    "# copy_df = copy_df[required_columns]\n",
    "# copy_df = copy_df[copy_df.sensor_type==\"Respeck\"] \n",
    "# copy_df.reset_index(drop=True, inplace=True)\n",
    "# print(only_respeck_df[only_respeck_df.recording_id==\"Respeck_s1541031_Falling on knees_06-10-2021_15-46-54\"])\n",
    "\n",
    "for rid, group in base_df.groupby(\"activity_code\"):\n",
    "    print(f\"Processing rid = {rid}\")\n",
    "    large_enough_windows = [window for window in group.rolling(window=window_size, min_periods=window_size) if len(window) == window_size]\n",
    "\n",
    "\n",
    "\n",
    "    overlapping_windows = large_enough_windows[::step_size] \n",
    "    # then we will append a window ID to each window\n",
    "    for window in overlapping_windows:\n",
    "        window.loc[:, 'window_id'] = window_number\n",
    "        window_number += 1\n",
    "\n",
    "\n",
    "    all_overlapping_windows.append(pd.concat(overlapping_windows).reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [timestamp, accel_x, accel_y, accel_z, gyro_x, gyro_y, gyro_z, sensor_type, activity_type, activity_code, subject_id, recording_id, window_id]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "final_sliding_windows = pd.concat(all_overlapping_windows).reset_index(drop=True)\n",
    "\n",
    "is_NaN = final_sliding_windows.isnull()\n",
    "row_has_NaN = is_NaN.any(axis=1)\n",
    "rows_with_NaN = final_sliding_windows[row_has_NaN]\n",
    "\n",
    "print(rows_with_NaN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to assign a number to each categorical class for the Random Forest Classifer. It is mainly up to you how you categorise your classes. In this example, we will use the labels:\n",
    "- 0 for Desk work\n",
    "- 1 for Walking at normal speed\n",
    "\n",
    "Bear in mind that your classification task will be multi-class, not binary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Climbing stairs': 0, 'Descending stairs': 1, 'Desk work': 2, 'Falling on knees': 3, 'Falling on the back': 4, 'Falling on the left': 5, 'Falling on the right': 6, 'Lying down left': 7, 'Lying down on back': 8, 'Lying down on stomach': 9, 'Lying down right': 10, 'Movement': 11, 'Running': 12, 'Sitting': 13, 'Sitting bent backward': 14, 'Sitting bent forward': 15, 'Standing': 16, 'Walking at normal speed': 17}\n"
     ]
    }
   ],
   "source": [
    "class_labels = {}\n",
    "label_to_activity = {}\n",
    "activities = sorted(final_sliding_windows.activity_type.unique())\n",
    "\n",
    "for idx, activity in enumerate(activities):\n",
    "    class_labels[activity] = idx\n",
    "    label_to_activity[idx] = activity\n",
    "\n",
    "print(class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activity_type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>window_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67302</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67303</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67304</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67305</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67306</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67307 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           activity_type\n",
       "window_id               \n",
       "0                     13\n",
       "1                     13\n",
       "2                     13\n",
       "3                     13\n",
       "4                     13\n",
       "...                  ...\n",
       "67302                 16\n",
       "67303                 16\n",
       "67304                 16\n",
       "67305                 16\n",
       "67306                 16\n",
       "\n",
       "[67307 rows x 1 columns]"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_id_class_labels = final_sliding_windows.groupby(\"window_id\")[['activity_type']].agg(np.min).replace(class_labels)\n",
    "window_id_class_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leave-one-subject-out cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the other reason our previous classifier functions so well is that each person performs activities in a very consistent manner. If a subject's data is both in the training set and the test set, it provides the model with an unfair advantage. Moreover, the results that your model will output will be falsely high. When you test your model on a completely new, unseen subject, your accuracy will drop considerably. \n",
    "\n",
    "This is why when training a HAR model you will want to do a special kind of cross-validation: Leave-One-Subject-Out (LOSOXV), where we leave one (or more) subject(s) in the testing set at each iteration.\n",
    "\n",
    "![losoxv](../Images/LOOCV.png \"losoxv\")\n",
    "\n",
    "This ensures that the results we get from our classifier are consistent to what we would get in real life, if we were to test the model on a new user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a simple CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some different preprocessing techniques you can apply when the resulting \"datapoint\" is an input to a convolutional neural network. \n",
    "\n",
    "You can use two types of convolutional layers:\n",
    "* 1D Conv Layers - which will work on 1D data, for example a single axis from one single sensor (accel, gyro or mag)\n",
    "* 2D Conv Layers - suitable if the input data is in the form of an image, for example\n",
    "\n",
    "We will be demonstrating how to build a simple 1D CNN using 6 channels: the 3 axes of the accelerometer and the 3 axes of the gyroscope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "filters = 64\n",
    "kernel_size = 3\n",
    "n_features = 6\n",
    "activation='relu'\n",
    "n_classes = len(class_labels)\n",
    "print(n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_21 (Conv1D)           (None, 48, 64)            1216      \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 48, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 48, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 46, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 46, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 46, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 44, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 44, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 44, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 2816)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 100)               281700    \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 18)                1818      \n",
      "=================================================================\n",
      "Total params: 310,206\n",
      "Trainable params: 309,822\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv1D(filters=filters, kernel_size=kernel_size, activation='linear', \n",
    "                 input_shape=(window_size, n_features)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(activation))\n",
    "\n",
    "model.add(Conv1D(filters=filters, kernel_size=kernel_size, activation='linear'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(activation))\n",
    "\n",
    "model.add(Conv1D(filters=filters, kernel_size=kernel_size, activation='linear'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(activation))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-generating the data in the appropriate format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to do a bit more work with our data to bring it into a format fit for training a CNN. \n",
    "\n",
    "A CNN will take multi-dimensional arrays as input. We have already specified that the input shape is (window_size, n_features), i.e. (50, 6). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that we generated sliding windows before. Now we just need to take the raw values from each window and create a training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['accel_x', 'accel_y', 'accel_z', 'gyro_x', 'gyro_y', 'gyro_z']"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_of_interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for window_id, group in final_sliding_windows.groupby('window_id'):\n",
    "#     print(f\"window_id = {window_id}\")\n",
    "    \n",
    "    shape = group[columns_of_interest].values.shape\n",
    "#     print(f\"shape = {shape}\")\n",
    "    \n",
    "    X.append(group[columns_of_interest].values)\n",
    "    y.append(class_labels[group[\"activity_type\"].values[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape = (26907, 50, 6)\n",
      "y shape = (26907,)\n"
     ]
    }
   ],
   "source": [
    "X = np.asarray(X)\n",
    "y = np.asarray(y)\n",
    "\n",
    "print(f\"X shape = {X.shape}\")\n",
    "print(f\"y shape = {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do a similar test/train split for demonstration purposes. Remember that you will have to split your data by subjects, not radomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.2, train_size=0.8)\n",
    "\n",
    "y_train = np.asarray(pd.get_dummies(y_train), dtype=np.float32)\n",
    "y_test = np.asarray(pd.get_dummies(y_test), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape = (21525, 50, 6)\n",
      "y_train shape = (21525, 18)\n",
      "X_test shape = (5382, 50, 6)\n",
      "y_test shape = (5382, 18)\n"
     ]
    }
   ],
   "source": [
    "print(f\"X_train shape = {X_train.shape}\")\n",
    "print(f\"y_train shape = {y_train.shape}\")\n",
    "\n",
    "print(f\"X_test shape = {X_test.shape}\")\n",
    "print(f\"y_test shape = {y_test.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to start the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "model.compile(\n",
    "    optimizer=optimizers.SGD(lr=0.01),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21525 samples\n",
      "Epoch 1/20\n",
      "21525/21525 [==============================] - 35s 2ms/sample - loss: 2.2701 - accuracy: 0.2645\n",
      "Epoch 2/20\n",
      "21525/21525 [==============================] - 35s 2ms/sample - loss: 1.7688 - accuracy: 0.4519\n",
      "Epoch 3/20\n",
      "21525/21525 [==============================] - 34s 2ms/sample - loss: 1.4265 - accuracy: 0.5511\n",
      "Epoch 4/20\n",
      "21525/21525 [==============================] - 32s 2ms/sample - loss: 1.2433 - accuracy: 0.6059\n",
      "Epoch 5/20\n",
      "21525/21525 [==============================] - 31s 1ms/sample - loss: 1.1339 - accuracy: 0.6300\n",
      "Epoch 6/20\n",
      "21525/21525 [==============================] - 35s 2ms/sample - loss: 1.0529 - accuracy: 0.6428\n",
      "Epoch 7/20\n",
      "21525/21525 [==============================] - 28s 1ms/sample - loss: 1.0024 - accuracy: 0.6562\n",
      "Epoch 8/20\n",
      "21525/21525 [==============================] - 29s 1ms/sample - loss: 0.9540 - accuracy: 0.6656\n",
      "Epoch 9/20\n",
      "21525/21525 [==============================] - 29s 1ms/sample - loss: 0.9165 - accuracy: 0.6736\n",
      "Epoch 10/20\n",
      "21525/21525 [==============================] - 28s 1ms/sample - loss: 0.8961 - accuracy: 0.6799\n",
      "Epoch 11/20\n",
      "21525/21525 [==============================] - 28s 1ms/sample - loss: 0.8645 - accuracy: 0.6916\n",
      "Epoch 12/20\n",
      "21525/21525 [==============================] - 36s 2ms/sample - loss: 0.8356 - accuracy: 0.7015\n",
      "Epoch 13/20\n",
      "21525/21525 [==============================] - 32s 2ms/sample - loss: 0.8181 - accuracy: 0.7084\n",
      "Epoch 14/20\n",
      "21525/21525 [==============================] - 32s 2ms/sample - loss: 0.8016 - accuracy: 0.7133\n",
      "Epoch 15/20\n",
      "21525/21525 [==============================] - 30s 1ms/sample - loss: 0.7813 - accuracy: 0.7183\n",
      "Epoch 16/20\n",
      "21525/21525 [==============================] - 35s 2ms/sample - loss: 0.7582 - accuracy: 0.7307\n",
      "Epoch 17/20\n",
      "21525/21525 [==============================] - 29s 1ms/sample - loss: 0.7446 - accuracy: 0.7332\n",
      "Epoch 18/20\n",
      "21525/21525 [==============================] - 30s 1ms/sample - loss: 0.7291 - accuracy: 0.7396\n",
      "Epoch 19/20\n",
      "21525/21525 [==============================] - 29s 1ms/sample - loss: 0.7162 - accuracy: 0.7418\n",
      "Epoch 20/20\n",
      "21525/21525 [==============================] - 29s 1ms/sample - loss: 0.7005 - accuracy: 0.7523\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f82a0670550>"
      ]
     },
     "execution_count": 497,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train,\n",
    "        batch_size=128, epochs=20, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now view the accuracy of our model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16  7 15 ...  4  4  5]\n"
     ]
    }
   ],
   "source": [
    "# stats\n",
    "y_pred_ohe = model.predict(X_test)\n",
    "y_pred_labels = np.argmax(y_pred_ohe, axis=1)\n",
    "y_true_labels = np.argmax(y_test, axis=1)\n",
    "print(y_pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "Classification report\n",
      "********************************************************************************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.30      0.41       272\n",
      "           1       0.60      0.44      0.51       296\n",
      "           2       0.73      0.28      0.41       277\n",
      "           3       0.83      0.72      0.77       283\n",
      "           4       0.86      0.80      0.82       299\n",
      "           5       0.79      0.85      0.82       299\n",
      "           6       0.81      0.78      0.80       286\n",
      "           7       0.75      0.86      0.80       293\n",
      "           8       0.81      0.85      0.83       317\n",
      "           9       0.82      0.76      0.79       307\n",
      "          10       0.84      0.87      0.86       318\n",
      "          11       0.75      0.45      0.56       332\n",
      "          12       0.93      0.91      0.92       266\n",
      "          13       0.35      0.37      0.36       270\n",
      "          14       0.68      0.83      0.75       309\n",
      "          15       0.63      0.70      0.67       297\n",
      "          16       0.52      0.64      0.57       297\n",
      "          17       0.41      0.79      0.54       364\n",
      "\n",
      "    accuracy                           0.68      5382\n",
      "   macro avg       0.71      0.68      0.68      5382\n",
      "weighted avg       0.71      0.68      0.68      5382\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"*\" * 80)\n",
    "print(\"Classification report\")\n",
    "print(\"*\" * 80)\n",
    "print(classification_report(y_true_labels, y_pred_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are now ready to start developing your own models for HAR. There are numerous tutorials online which you can follow to build models like LSTMs, CNNs, RFCs and many others. \n",
    "\n",
    "You have a wide choice of ways to solve this classification model. Here are a few things to think about:\n",
    "\n",
    "* What type of preprocessing do you want to apply to your data? Examples include:\n",
    "    * smoothing the sensor axes\n",
    "    * performing axis fusion\n",
    "    * extracting scalograms from the signal\n",
    "    * manually extracting features from the signal\n",
    "    * choosing to leave out certain axes\n",
    "\n",
    "* What type of model do you want to train?\n",
    "    * simple ML model\n",
    "    * deep learning model\n",
    "    \n",
    "* Do you want a hierarchical model or a flat model?\n",
    "    * hierarchical models means you don't have to train the same type of model for each activity\n",
    "    * a flat model might be faster to train and apply in real time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_directory = './models/'\n",
    "current_model_path = models_directory + 'CNN_model_HARv_v1/'\n",
    "\n",
    "tflite_model_filename = 'CNN_HAR_v1.tflite'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/CNN_model_HARv_v1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/CNN_model_HARv_v1/assets\n"
     ]
    }
   ],
   "source": [
    "#Save original model first. We will use the SavedModel to convert it to TFLite as recommended by the Tensorflow documentation.\n",
    "tf.saved_model.save(model, current_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert SavedModel to TFLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the model\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(current_model_path) # path to the SavedModel directory\n",
    "tflite_model = converter.convert()\n",
    "tflite_model_filename = 'CNN_HAR_v1.tflite'\n",
    "# Save the model.\n",
    "with open(current_model_path+tflite_model_filename, 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Test on Python - Tensorflow Lite\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape\n",
      "[ 1 50  6]\n",
      "See Test Shape\n",
      "(1, 50, 6)\n",
      "Current output results\n",
      "[6.2270577e-05 2.4624990e-06 1.5130171e-01 3.9093671e-04 5.0324547e-06\n",
      " 7.4569823e-04 3.8591963e-05 1.3974484e-03 2.7534372e-06 2.3628731e-07\n",
      " 1.2048521e-04 8.6326441e-03 9.0560906e-07 2.8892693e-01 3.9909825e-02\n",
      " 4.7166407e-01 3.6739558e-02 5.8311980e-05]\n",
      "18\n",
      "Which is the most confident?\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "# Load TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=current_model_path+tflite_model_filename)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Test model on random input data.\n",
    "input_shape = input_details[0]['shape']\n",
    "print(\"Input Shape\")\n",
    "print(input_shape)\n",
    "test = X_test.astype(np.float32)\n",
    "#Test data to feed as parameter\n",
    "test_part = test[2:3]\n",
    "print(\"See Test Shape\")\n",
    "print(test_part.shape)\n",
    "\n",
    "interpreter.set_tensor(input_details[0]['index'], test_part)\n",
    "\n",
    "interpreter.invoke()\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "output_results = np.squeeze(output_data)\n",
    "\n",
    "print(\"Current output results\")\n",
    "print(output_results)\n",
    "print(len(output_results))\n",
    "\n",
    "print(\"Which is the most confident?\")\n",
    "max_index = np.argmax(output_results, axis=0)\n",
    "\n",
    "print(max_index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
